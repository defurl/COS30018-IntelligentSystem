# Week 11: RNNs, LSTM & Transformers

## ðŸ“Œ Core Concept

Architectures designed to process sequential data.

---

## ðŸ“Š Sequence Data Types

| Type            | Examples              |
| --------------- | --------------------- |
| **Text**        | Sentences, documents  |
| **Audio**       | Speech, music         |
| **Time Series** | Stock prices, weather |
| **Video**       | Frame sequences       |

---

## ðŸ”„ Recurrent Neural Networks (RNN)

### Architecture

```
    â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”
xâ‚ â†’â”‚ hâ‚ â”‚â†’ â”‚ hâ‚‚ â”‚â†’ â”‚ hâ‚ƒ â”‚â†’ â”‚ hâ‚„ â”‚â†’ y
    â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”˜
      â†“        â†“        â†“        â†“
     yâ‚       yâ‚‚       yâ‚ƒ       yâ‚„
```

### Update Equations

```
hâ‚œ = tanh(W_h Â· xâ‚œ + W_hh Â· hâ‚œâ‚‹â‚ + b)
yâ‚œ = W_y Â· hâ‚œ
```

### Limitations

- Vanishing/exploding gradients
- Limited long-term memory
- Sequential processing (no parallelization)

---

## ðŸ§  LSTM (Long Short-Term Memory)

### Key Innovation: Cell State

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Cell State (long-term)
    â”‚           â”‚           â”‚
    â•³           â•³           â•³
 Forget      Input       Output
  Gate        Gate         Gate
```

### Gates

| Gate       | Function                    |
| ---------- | --------------------------- |
| **Forget** | What to discard from memory |
| **Input**  | What new info to store      |
| **Output** | What to output              |

---

## ðŸ”€ GRU (Gated Recurrent Unit)

Simplified LSTM with fewer parameters:

- **Reset gate**: How much past to forget
- **Update gate**: How much to update hidden state

```
LSTM: 3 gates + cell state
GRU:  2 gates (simpler, faster)
```

---

## âš¡ Transformer Architecture

### Key Innovation: Self-Attention

"Attention Is All You Need" (Vaswani et al., 2017)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TRANSFORMER               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚   ENCODER   â”‚  â”‚   DECODER   â”‚   â”‚
â”‚ â”‚             â”‚â†’ â”‚             â”‚   â”‚
â”‚ â”‚ Self-Attn   â”‚  â”‚ Self-Attn   â”‚   â”‚
â”‚ â”‚ Feed-Fwd    â”‚  â”‚ Cross-Attn  â”‚   â”‚
â”‚ â”‚             â”‚  â”‚ Feed-Fwd    â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ” Self-Attention Mechanism

### Query-Key-Value

```
Attention(Q, K, V) = softmax(QKáµ€/âˆšd) Â· V
```

### Intuition

```
"The cat sat on the mat because it was tired"
                              â”‚
                  What does "it" refer to?
                              â†“
              Attention connects "it" to "cat"
```

---

## ðŸ“ Positional Encoding

Since transformers process all positions simultaneously:

```
Position info added via:
PE(pos, 2i)   = sin(pos/10000^(2i/d))
PE(pos, 2i+1) = cos(pos/10000^(2i/d))
```

---

## ðŸ†š Architecture Comparison

| Aspect          | RNN  | LSTM   | Transformer |
| --------------- | ---- | ------ | ----------- |
| Parallelization | âŒ   | âŒ     | âœ…          |
| Long-range deps | âŒ   | âœ…     | âœ…âœ…        |
| Training speed  | Slow | Slow   | Fast        |
| Memory          | Low  | Medium | High        |

---

## ðŸŒ Transformer Applications

```mermaid
graph TB
    T[Transformer]
    T --> NLP[NLP: GPT, BERT, LLaMA]
    T --> Vision[Vision: ViT, DINO]
    T --> Audio[Audio: Whisper]
    T --> Multi[Multimodal: GPT-4V, Gemini]
```

---

## ðŸŽ¯ Foundation for LLMs

- **Encoder-only**: BERT (understanding)
- **Decoder-only**: GPT (generation)
- **Encoder-Decoder**: T5, BART (translation)

---

## ðŸ“š Key Takeaways

1. RNNs process sequences but suffer from gradient issues
2. LSTM/GRU solve vanishing gradients with gating
3. Transformers enable parallel processing via attention
4. Self-attention captures long-range dependencies
5. Transformers are the foundation of modern AI
