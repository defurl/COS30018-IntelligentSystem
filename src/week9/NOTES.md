# Week 9: Reinforcement Learning

## ğŸ“Œ Core Concept

RL trains agents to make sequential decisions by learning from rewards.

---

## ğŸ® RL Framework

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”   action aâ‚œ    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚    â”‚ AGENT â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ENVIRONMENTâ”‚â”‚
â”‚    â”‚       â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”˜  state sâ‚œâ‚Šâ‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚               reward râ‚œâ‚Šâ‚              â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Key Components

| Component   | Symbol | Description              |
| ----------- | ------ | ------------------------ |
| **State**   | s âˆˆ S  | Current situation        |
| **Action**  | a âˆˆ A  | Agent's choice           |
| **Reward**  | r      | Immediate feedback       |
| **Policy**  | Ï€(s)   | State â†’ Action mapping   |
| **Value**   | V(s)   | Expected future reward   |
| **Q-value** | Q(s,a) | Value of action in state |

---

## ğŸ¯ Goal

Find optimal policy Ï€\* that maximizes expected cumulative reward:

```
R = râ‚€ + Î³râ‚ + Î³Â²râ‚‚ + Î³Â³râ‚ƒ + ...

where Î³ = discount factor (0 < Î³ < 1)
```

---

## ğŸ“ Bellman Equation

### State Value Function

```
V^Ï€(s) = r(s) + Î³ Â· Î£ P(s'|s, Ï€(s)) Â· V^Ï€(s')
```

### Optimal Value

```
V*(s) = r(s) + Î³ Â· max_a Î£ P(s'|s,a) Â· V*(s')
```

---

## ğŸ§® Solution Methods

```mermaid
graph TB
    RL[RL Methods]
    RL --> MB[Model-Based]
    RL --> MF[Model-Free]

    MB --> DP[Dynamic Programming]
    MF --> MC[Monte Carlo]
    MF --> TD[Temporal Difference]

    TD --> SARSA
    TD --> QL[Q-Learning]
```

---

## ğŸ“Š Method Comparison

| Method                  | Requires Model | Updates        |
| ----------------------- | -------------- | -------------- |
| **Dynamic Programming** | Yes            | All states     |
| **Monte Carlo**         | No             | End of episode |
| **TD Learning**         | No             | Every step     |

---

## ğŸ² Q-Learning

### Update Rule

```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max_a' Q(s',a') - Q(s,a)]

where:
  Î± = learning rate
  Î³ = discount factor
```

### Algorithm

```
1. Initialize Q(s,a) arbitrarily
2. For each episode:
   a. Initialize state s
   b. For each step:
      - Choose action a (Îµ-greedy)
      - Take action, observe r, s'
      - Update Q(s,a)
      - s â† s'
```

---

## ğŸ” Exploration vs Exploitation

### Îµ-Greedy Policy

```
a = { argmax_a Q(s,a)  with prob (1-Îµ)
    { random action     with prob Îµ
```

---

## ğŸŒ Applications

| Domain   | Example                  |
| -------- | ------------------------ |
| Games    | Chess, Go, Atari         |
| Robotics | Navigation, manipulation |
| Trading  | Portfolio optimization   |
| Control  | Autonomous vehicles      |

---

## ğŸ› ï¸ Getting Started

- **OpenAI Gym**: Standard RL environments
- **Stable Baselines**: Pre-built RL algorithms
- Classic problems: CartPole, MountainCar, LunarLander
