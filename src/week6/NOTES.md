# Week 6: Artificial Neural Networks & Deep Learning

## ğŸ“Œ Core Concept

Neural networks are computational models inspired by biological neurons.

---

## ğŸ§  Single Neuron (Perceptron)

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
xâ‚ â”€â”€wâ‚â”€â”€â–ºâ”‚         â”‚
xâ‚‚ â”€â”€wâ‚‚â”€â”€â–ºâ”‚  Î£ â†’ f  â”‚â”€â”€â–º y
xâ‚ƒ â”€â”€wâ‚ƒâ”€â”€â–ºâ”‚         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†‘
           b (bias)

y = f(Î£ wáµ¢xáµ¢ + b)
```

---

## âš¡ Activation Functions

| Function    | Formula           | Use Case           |
| ----------- | ----------------- | ------------------ |
| **Sigmoid** | 1/(1+eâ»á¶»)         | Binary output      |
| **Tanh**    | (eá¶»-eâ»á¶»)/(eá¶»+eâ»á¶») | Hidden layers      |
| **ReLU**    | max(0, z)         | Most common        |
| **Softmax** | eá¶»â±/Î£eá¶»Ê²          | Multi-class output |

---

## ğŸ—ï¸ Network Architecture

```
Input Layer    Hidden Layers    Output Layer
    â—‹              â—‹  â—‹              â—‹
    â—‹              â—‹  â—‹              â—‹
    â—‹      â†’       â—‹  â—‹      â†’       â—‹
    â—‹              â—‹  â—‹
    â—‹              â—‹  â—‹
  (n=400)       (layers)          (k=10)
```

---

## ğŸ“‰ Training Process

### Forward Pass

```
Z[l] = W[l] Â· A[l-1] + b[l]
A[l] = g(Z[l])
```

### Cost Function (Cross-Entropy)

```
J(W) = -(1/m) Î£ [yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
```

### Backpropagation

```
âˆ‚J/âˆ‚W[l] = (âˆ‚J/âˆ‚A[l]) Â· (âˆ‚A[l]/âˆ‚Z[l]) Â· (âˆ‚Z[l]/âˆ‚W[l])
```

---

## ğŸ”„ Gradient Descent

```
Repeat until convergence:
    W := W - Î± Â· âˆ‚J/âˆ‚W
    b := b - Î± Â· âˆ‚J/âˆ‚b
```

---

## ğŸ“Š Multiclass Classification

### Softmax Output

```
Å·áµ¢ = eá¶»â± / Î£â±¼ eá¶»Ê²

Î£ Å·áµ¢ = 1 (probabilities sum to 1)
```

### Loss (Categorical Cross-Entropy)

```
L = -Î£ yáµ¢ Â· log(Å·áµ¢)
```

---

## ğŸš€ Why Deep Learning Now?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Better algorithms & understanding â”‚
â”‚ â€¢ Computing power (GPUs, TPUs)     â”‚
â”‚ â€¢ Big data availability            â”‚
â”‚ â€¢ Open-source tools & models       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Frameworks

| Framework        | Organization | Language |
| ---------------- | ------------ | -------- |
| **TensorFlow**   | Google       | Python   |
| **PyTorch**      | Meta         | Python   |
| **Keras**        | (TF wrapper) | Python   |
| **Scikit-learn** | Community    | Python   |

---

## âš ï¸ Neural Network Challenges

| Issue                   | Solution                   |
| ----------------------- | -------------------------- |
| **Vanishing gradients** | ReLU, batch norm           |
| **Overfitting**         | Dropout, regularization    |
| **Black box**           | Explainable AI techniques  |
| **Computational cost**  | GPUs, distributed training |
