# Week 5: Classification & Generative Models

## ğŸ“Œ Core Concepts

This week covers classification algorithms: discriminative vs generative approaches.

---

## ğŸ¯ Discriminative vs Generative

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DISCRIMINATIVE    â”‚     GENERATIVE      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Models P(y|x)       â”‚ Models P(x|y)Â·P(y)  â”‚
â”‚ Learn decision      â”‚ Learn class         â”‚
â”‚ boundary directly   â”‚ distributions       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Examples:           â”‚ Examples:           â”‚
â”‚ - Logistic Reg      â”‚ - Naive Bayes       â”‚
â”‚ - SVM               â”‚ - GDA               â”‚
â”‚ - Neural Networks   â”‚ - HMMs              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Logistic Regression

### Sigmoid Function

```
Ïƒ(z) = 1 / (1 + eâ»á¶»)

where z = wÂ·x + b
```

### Prediction

```
P(y=1|x) = Ïƒ(wÂ·x + b)
```

### Loss Function (Binary Cross-Entropy)

```
L = -[yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
```

---

## ğŸ”” Gaussian Discriminant Analysis (GDA)

### Assumptions

- P(x|y=0) ~ N(Î¼â‚€, Î£)
- P(x|y=1) ~ N(Î¼â‚, Î£)

### Classification via Bayes Rule

```
P(y=1|x) = P(x|y=1)Â·P(y=1) / P(x)
```

---

## ğŸ² NaÃ¯ve Bayes

### Key Assumption

Features are conditionally independent given class:

```
P(xâ‚,xâ‚‚,...,xâ‚™|y) = âˆ P(xáµ¢|y)
```

### Classification

```
Å· = argmax P(y) Â· âˆ P(xáµ¢|y)
         y
```

---

## ğŸ“§ NaÃ¯ve Bayes for Text (Spam Detection)

```mermaid
graph LR
    E[Email] --> FV[Feature Vector]
    FV --> |"word âˆˆ email"| NB[NaÃ¯ve Bayes]
    NB --> S[Spam?]
```

### Feature Vector Example

```
x = [1, 0, 0, 1, ...]
    â”‚     â”‚
    â”‚     â””â”€â”€ "aardvark" NOT present
    â””â”€â”€ "a" IS present
```

---

## ğŸ“ PCA (Principal Component Analysis)

### Purpose

Dimensionality reduction while preserving variance.

### Process

```
1. Standardize data
2. Compute covariance matrix
3. Find eigenvectors/eigenvalues
4. Project onto top k components
```

### Visualization

```
High-dim data â†’ PCA â†’ Low-dim representation
   (n features)         (k < n features)
```

---

## âš–ï¸ GDA vs Logistic Regression

| Aspect              | GDA                    | Logistic Regression |
| ------------------- | ---------------------- | ------------------- |
| **Assumptions**     | Gaussian distributions | None on P(x\|y)     |
| **Data efficiency** | Better with less data  | Needs more data     |
| **Robustness**      | Sensitive to outliers  | More robust         |
| **Flexibility**     | Less flexible          | More flexible       |

---

## ğŸ¯ Project Relevance

- **Traffic Flow**: Classify traffic states
- **Pattern Recognition**: Identify congestion patterns
